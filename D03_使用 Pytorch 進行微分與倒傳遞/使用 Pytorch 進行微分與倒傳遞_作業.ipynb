{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxtArZ7u16sr"
   },
   "source": [
    "### 作業目標: 使用Pytorch進行微分與倒傳遞\n",
    "這份作業我們會實作微分與倒傳遞以及使用Pytorch的Autograd。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_BwC7sg16ss"
   },
   "source": [
    "### 使用Pytorch實作微分與倒傳遞\n",
    "\n",
    "這裡我們很簡單的實作兩層的神經網路進行回歸問題，其中loss function為L2 loss\n",
    "\n",
    "$$\n",
    "L2\\_loss = (y_{pred}-y)^2\n",
    "$$\n",
    "\n",
    "兩層經網路如下所示\n",
    "$$\n",
    "y_{pred} = ReLU(XW_1)W_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ocsA8ch-16st"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "o2v8hkG616sz",
    "outputId": "0b737d18-59c2-4bb7-f541-e0ca6ab51a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41974892.0\n",
      "50 15187.625\n",
      "100 756.4373168945312\n",
      "150 63.35027313232422\n",
      "200 6.909194469451904\n",
      "250 0.9250452518463135\n",
      "300 0.1443679928779602\n",
      "350 0.02511386200785637\n",
      "400 0.004950829781591892\n",
      "450 0.001248681452125311\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "# 初始化weight W1, W2\n",
    "\n",
    "W1 = torch.randn((D_in, H), requires_grad=True).to(device)\n",
    "W2 = torch.randn((H, D_out), requires_grad=True).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "    # 向前傳遞: 計算y_pred\n",
    "    h = torch.matmul(x, W1)\n",
    "    h_relu = torch.relu(h)\n",
    "    y_pred = torch.matmul(h_relu, W2)\n",
    "\n",
    "    # 計算loss\n",
    "    loss = torch.square( y_pred - y).sum()\n",
    "    if t % 50 == 0 :\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "    y_pred_grad = 2 *(y_pred - y)\n",
    "    W2_grad = h_relu.T.matmul(y_pred_grad)\n",
    "    h_grad = y_pred_grad.matmul(W2.T) * (h > 0)\n",
    "    W1_grad = x.T.matmul(h_grad)\n",
    "\n",
    "    W2_grad_test = h_relu.T.matmul(y_pred_grad)\n",
    "    h_grad_test = y_pred_grad.matmul(W2.T) * (h > 0)\n",
    "    W1_grad_test = x.T.matmul(h_grad)\n",
    "    \n",
    "    # 參數更新\n",
    "    W1.data -= learning_rate * W1_grad\n",
    "    W2.data -= learning_rate * W2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9XiShaU16s3"
   },
   "source": [
    "### 使用Pytorch的Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_VP1YW7516s4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dlj3NwsP16s6",
    "outputId": "0463fd34-3edf-4516-9d36-c1143463790d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25384656.0\n",
      "50 7066.51611328125\n",
      "100 159.8881378173828\n",
      "150 6.105778694152832\n",
      "200 0.28546658158302307\n",
      "250 0.014956336468458176\n",
      "300 0.0010549200233072042\n",
      "350 0.0001698558626230806\n",
      "400 5.5406046158168465e-05\n",
      "450 2.6562072889646515e-05\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "W1 = torch.randn((D_in, H), requires_grad = True).to(device)\n",
    "W2 = torch.randn((H, D_out), requires_grad = True).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "    # 向前傳遞: 計算y_pred\n",
    "    h = torch.matmul(x, W1)\n",
    "    h_relu = torch.relu(h)\n",
    "    y_pred = torch.matmul(h_relu, W2)\n",
    "\n",
    "    # 計算loss\n",
    "    loss = torch.square(y_pred - y).sum()\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "    loss.backward()\n",
    "\n",
    "    # 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "    # 更新參數W1 W2\n",
    "        W1.data -= learning_rate * W1.grad\n",
    "        W2.data -= learning_rate * W2.grad\n",
    "\n",
    "    # 將紀錄的gradient清空(因為已經更新參數)\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znJFnEdr16s9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "微分與倒傳遞作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
