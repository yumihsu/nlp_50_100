{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "* Base class for all neural network modules\n",
    "* 只要在nn.Module的子類中定義了forward函數，backward函數就會被自動實現（利用Autograd）\n",
    "* nn.Conv2d 本身也是nn.Module的類別(此時我們可以先不用理解nn.Conv2D做了什麼，只需了解其包含一些參數與操作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實踐 forward propagation \n",
    "* 為什麼不應該直接call model.forward : https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn(1,1,124,124)\n",
    "output = model(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看 model 底下的 modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model.modules 遞迴的列出所有的 modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n",
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model.children 只列出第一層的子 modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for module in model.children():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看 model 內的 parameters (torch.nn.parameter.Parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .named_parameters\n",
    "* named_parameters會列出每個nn.Module底下parameters 的名字,數值\n",
    "* 同時可以查看 requires_grad是否開啟(for backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "conv1.bias True\n",
      "conv2.weight True\n",
      "conv2.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name,param.requires_grad)\n",
    "    #param.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .parameters\n",
    "* 不會印出名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([20, 1, 5, 5]) True\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([20]) True\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([20, 20, 5, 5]) True\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([20]) True\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param),param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 計算模型可訓練參數總量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總共參數量： 10540\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('總共參數量：' ,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn(1,1,124,124)\n",
    "output = model(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認 requires_grad為 True (default 就是 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "conv1.bias True\n",
      "conv2.weight True\n",
      "conv2.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此時還沒做backpropagation，parameters沒有gradient value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 執行backward，完成後就能看到每個parameters底下的gradient value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  605.5430,  -376.2526,  -264.9802,  -405.8641,   330.9492],\n",
      "          [ -498.8722,    89.9731,   254.4219,  -252.9127,   -92.0358],\n",
      "          [  -63.3486,  -108.4003,    63.5249,  -241.9144,  -187.6409],\n",
      "          [ -306.5060,  -275.6410,   458.6197,    81.5387,   455.1625],\n",
      "          [ -405.7625,  -175.4588,  -278.4715,   376.6024,   386.9280]]],\n",
      "\n",
      "\n",
      "        [[[   10.3513,   -12.5419,    77.3492,   103.1113,   -32.1752],\n",
      "          [  209.9700,  -101.2162,   -43.0707,   385.9092,   -51.0152],\n",
      "          [ -268.2236,   238.1764,   337.8172,   247.8170,  -107.5047],\n",
      "          [ -219.5956,    -3.0727,  -244.3324,  -122.1673,  -173.0466],\n",
      "          [   43.3710,    58.4297,   166.3651,     6.5298,    90.7496]]],\n",
      "\n",
      "\n",
      "        [[[  483.3836,   305.7553,  -458.8383,  -221.6854,   602.1165],\n",
      "          [  583.9101,   778.7874,  -209.8309,  -612.6638,   516.8854],\n",
      "          [  416.4789,  -699.1741,  -945.6849,  -216.3947,  -673.4405],\n",
      "          [  819.1714,  -201.8294,   406.1374,  -319.3001,   429.3490],\n",
      "          [  602.2468,  -678.9810,  -693.9678,   752.1765,   784.0652]]],\n",
      "\n",
      "\n",
      "        [[[  809.0253,   604.4544,   -87.2985,  -578.2562,  -544.4000],\n",
      "          [ -440.7764,   585.0804,   402.0968,  -374.8540,  -614.7849],\n",
      "          [  476.1155,  -720.7941,  -586.9445,  -764.1970,  -830.1735],\n",
      "          [ 1064.2527,   628.7438,   355.3343,   378.1860,  -883.1526],\n",
      "          [ -505.9547,  -319.7902,  -483.5060,   780.8324,   -83.9267]]],\n",
      "\n",
      "\n",
      "        [[[ -737.0930,  -401.4311,   997.4532,  -388.0460,  -478.0656],\n",
      "          [  109.8479,  -266.0916,   658.6634,   405.6873,  -498.4637],\n",
      "          [  308.9955,  1006.1117,  -840.5269,  1088.5605,   796.6777],\n",
      "          [ -656.4726,  -858.1002,  -636.3616,   939.9742,  1074.5929],\n",
      "          [-1001.9890,  1095.0132,   435.9132,  -401.5485,  -849.0075]]],\n",
      "\n",
      "\n",
      "        [[[  144.9306,   -89.9036,    93.0050,   141.6728,  -232.9626],\n",
      "          [  -68.9719,    63.0849,   151.2833,  -125.1994,    58.1559],\n",
      "          [  -23.2261,  -345.0825,   104.6869,   -98.0757,   160.3861],\n",
      "          [   -4.8536,   -64.4883,    91.1764,     7.1523,     4.9593],\n",
      "          [ -175.6973,   124.2332,   -29.1857,  -242.5546,    70.7612]]],\n",
      "\n",
      "\n",
      "        [[[  979.8315,  1171.6486,  1138.8428,  -202.4458,   -44.0875],\n",
      "          [  535.0835,   359.0179,  -515.8651,  1193.6562,   641.8873],\n",
      "          [  601.7836, -1212.3418, -1328.9573,   268.9213,  1145.6732],\n",
      "          [   26.3448, -1021.7304,  -212.7111,  -185.1251,  -767.8420],\n",
      "          [ -178.6742,  -829.2050,   647.8449,   632.2877,   753.5751]]],\n",
      "\n",
      "\n",
      "        [[[  622.7061,    -2.3930,  -148.9672,    50.8980,   -81.0619],\n",
      "          [   84.5059,   397.2386,    21.0169,   143.8800,   152.2065],\n",
      "          [ -477.9229,    -6.2040,  -262.6660,   149.9282,   -11.1991],\n",
      "          [ -440.5600,  -406.8486,    18.7126,  -473.1555,  -574.1703],\n",
      "          [  345.8516,   129.7010,   -21.3415,   -34.7926,   425.0724]]],\n",
      "\n",
      "\n",
      "        [[[ -479.6780,  -549.0784,   821.9579,   221.5721,  -341.7968],\n",
      "          [ -115.9460,   938.5527,    19.4280,  -121.6633,  -818.9929],\n",
      "          [ -727.3368,  1133.6417,   -53.0432,  -917.7423,   757.7192],\n",
      "          [  634.1412,   586.2394,   862.2001,  -437.1263,  -967.7211],\n",
      "          [ -188.3995,   835.9055,   536.9382,   715.1700,  -112.5441]]],\n",
      "\n",
      "\n",
      "        [[[ -216.8964,   -97.2162,    47.9547,   106.6763,   331.9647],\n",
      "          [  103.4478,  -453.6119,   109.1485,   -52.1252,   194.6870],\n",
      "          [    1.5818,  -296.0525,    76.5814,   220.9114,  -299.1609],\n",
      "          [  -93.9343,   289.7562,  -106.9446,  -415.1808,   336.1364],\n",
      "          [  -61.0344,   348.0795,   192.1247,     5.0987,    58.5242]]],\n",
      "\n",
      "\n",
      "        [[[ 1489.3082,   940.7023, -1310.7351, -1196.4915,  1378.6484],\n",
      "          [  931.7282,  -877.4978,  -204.4882,  -446.4377,   721.1833],\n",
      "          [ -228.7715,  -633.0731,  -225.0877,  -898.7388,    19.0303],\n",
      "          [ -932.7730,  -562.7511,  1306.7866,   -72.9404,  1075.1750],\n",
      "          [  637.8002,  -514.4822,   154.1132, -1034.0206,   866.6852]]],\n",
      "\n",
      "\n",
      "        [[[  -33.6553,   213.2183,  -184.5643,  -213.6052,   -13.2161],\n",
      "          [  186.9467,   578.8539,   216.1044,  -179.0983,  -573.4319],\n",
      "          [ -439.3163,   570.9538,  -330.6959,  -352.0945,    85.5601],\n",
      "          [ -262.9968,  -377.5460,  -587.9518,   651.3624,   470.3906],\n",
      "          [ -584.4456,   494.8559,    42.4345,   228.1978,  -245.7021]]],\n",
      "\n",
      "\n",
      "        [[[-1279.7773,   636.6422,   -76.3414, -1215.1307,   403.0977],\n",
      "          [  628.6918,  1506.5948,  -780.1583,  1034.4346,   298.8909],\n",
      "          [   33.9494,  -569.5017, -1203.5793,  1138.4991,   724.1384],\n",
      "          [-1337.6240,   902.7394,   566.6809,   990.4344,  -490.9751],\n",
      "          [ -880.1973,  1063.7368,   847.6312,   393.5496,  -495.2277]]],\n",
      "\n",
      "\n",
      "        [[[ -765.4339,  -384.2918,   720.4587,  -337.9025,  -255.9748],\n",
      "          [ -109.9123,  -925.4333,   360.6475,  -814.7787,  -412.7057],\n",
      "          [  374.9690,   549.3624,  -417.4434,  -363.9013,   879.9850],\n",
      "          [  294.8189,   -80.9110,  -281.6073,   422.3183,   319.2480],\n",
      "          [  541.4263,   350.1446,  -121.3475,  -383.6266,   680.9367]]],\n",
      "\n",
      "\n",
      "        [[[  205.8199,   173.3516,    -5.8419,   179.2454,   511.1621],\n",
      "          [  247.2435,  -311.5247,  -277.6038,  -335.7383,   135.7738],\n",
      "          [  266.9422,  -195.5945,   407.7334,   158.9191,    84.7256],\n",
      "          [   95.9708,   -57.5268,  -168.5162,    23.6547,   233.1278],\n",
      "          [ -181.2202,   109.1140,  -115.8165,  -317.6712,  -102.0948]]],\n",
      "\n",
      "\n",
      "        [[[ -195.2387,   214.8239,   -75.9606,   -90.7655,    21.7441],\n",
      "          [   82.0830,   122.1208,   -28.0643,   165.5058,   241.4978],\n",
      "          [  327.6100,   -63.9319,   -37.2555,   109.5804,     4.0427],\n",
      "          [   -9.1861,   104.0638,   -31.8985,   449.9451,   -75.7802],\n",
      "          [  258.5131,     8.4322,   -52.9064,    13.3709,    35.8771]]],\n",
      "\n",
      "\n",
      "        [[[  194.3337,    63.7799,   856.8578,   353.1023,  -175.5365],\n",
      "          [  316.7220,  -944.1069,   244.9676,   -72.2648,   517.9984],\n",
      "          [  260.0216,   100.2423,  -522.4292,  -270.8383,   -96.4711],\n",
      "          [  876.5643,   439.8040,   -35.0319,   682.0773,   774.9926],\n",
      "          [  224.6558,  -775.9289,  -240.6416,   207.4058, -1038.3156]]],\n",
      "\n",
      "\n",
      "        [[[   -7.3570,   -10.1217,  -442.9631,  -174.6045,   203.1513],\n",
      "          [  429.9394,   -55.7286,   123.8201,    97.1474,  -132.7810],\n",
      "          [  -39.9145,  -591.7169,   405.2292,  -334.7168,  -276.2837],\n",
      "          [  250.5756,  -453.2542,   309.9827,   290.9188,   148.5177],\n",
      "          [  -57.6414,    -6.4659,   203.5957,   144.7082,   167.9002]]],\n",
      "\n",
      "\n",
      "        [[[ -495.8257,   411.0639,  -492.8601,   192.3850,   293.7924],\n",
      "          [  478.6102,   876.9556,  -177.8542,   758.8286,   166.3032],\n",
      "          [ -427.7967,  -306.9132,  -398.8088,   509.0024,  -290.6655],\n",
      "          [  -63.4606,  -547.5717,  -724.2075,  -196.8200,  -180.2639],\n",
      "          [ -609.5925,  -676.9659,  -476.7245,   738.6589,  -105.0620]]],\n",
      "\n",
      "\n",
      "        [[[-1223.9213,  1192.8652, -1325.5128, -1273.8220,  -766.1549],\n",
      "          [  177.8565,   297.8636, -1415.8702,   575.6241,  -959.1609],\n",
      "          [  299.5808,   -23.4894,  -974.3489, -1007.6671,  -819.9911],\n",
      "          [ -200.6881,  1429.1027,  -381.0577,  -981.4630, -1154.5800],\n",
      "          [  559.7031,  -465.1191,  -820.3980,  1061.4374,   121.9745]]]])\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 當我們把 parameters 的 requires_grad關閉時，就無法成功的完成backward\n",
    "* 什麼時候會關閉requires_grad關閉時？ prediction (inference)的階段\n",
    "* 設定 requires_grad = True 是為了之後要做 backpropagation，在計算每個paramters的 gradient時，我們在forward propagation時需要保留額外的訊息(根據chain rule)，這會導致記憶體使用量上升與計算速度下降，然而只有在 training 階段時我們材需要做backpropagation，在 prediction (inference)的階段，我們則可以設定 requires_grad = False 來提升速度與降低記憶體使用量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn(1,1,124,124)\n",
    "output = model(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xuyouming/nlp_50_100/D07_搭建 PyTorch 神經網路基礎模型/搭建 PyTorch 神經網路基礎模型_練習.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xuyouming/nlp_50_100/D07_%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B/%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B_%E7%B7%B4%E7%BF%92.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_nlp/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_nlp/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "output.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with torch.no_grad()\n",
    "* 此行底下的requires_grad都會關閉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xuyouming/nlp_50_100/D07_搭建 PyTorch 神經網路基礎模型/搭建 PyTorch 神經網路基礎模型_練習.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xuyouming/nlp_50_100/D07_%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B/%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B_%E7%B7%B4%E7%BF%92.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m input_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m124\u001b[39m,\u001b[39m124\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xuyouming/nlp_50_100/D07_%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B/%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B_%E7%B7%B4%E7%BF%92.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m output \u001b[39m=\u001b[39m model(input_)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xuyouming/nlp_50_100/D07_%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B/%E6%90%AD%E5%BB%BA%20PyTorch%20%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E7%A4%8E%E6%A8%A1%E5%9E%8B_%E7%B7%B4%E7%BF%92.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_nlp/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch_nlp/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "with torch.no_grad():\n",
    "    input_ = torch.randn(1,1,124,124)\n",
    "    output = model(input_)\n",
    "    output.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讓我們自行搭建一個 nn.Module 並試算gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.x = torch.nn.Parameter(torch.tensor(2.4,dtype=torch.float32))\n",
    "        self.y = torch.nn.Parameter(torch.tensor(4.3,dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x*self.x**2 + x*self.y + x # 可以看成 output = w*x*x + w*y+2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x 的 gradient : 6.240000247955322\n",
      "self.y 的 gradient : 1.2999999523162842\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "input_ = torch.tensor(1.3, dtype = torch.float32)\n",
    "output = model(input_)\n",
    "output.backward()\n",
    "# output 對 self.x 的偏微分為 2 * w * x = 2 * 1.3 * 2.4 = 6.24 \n",
    "print('self.x 的 gradient : {}'.format(model.x.grad))\n",
    "# output 對 self.y 的偏微分為 w = 1.3\n",
    "print('self.y 的 gradient : {}'.format(model.y.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "* nn.Module 的容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Sequential(\n",
    "                        nn.Conv2d(3,\n",
    "                                  20,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1,\n",
    "                                  bias=False), \n",
    "                        nn.BatchNorm2d(20),\n",
    "                        nn.LeakyReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight True\n",
      "1.weight True\n",
      "1.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in layer.named_parameters():\n",
    "    print(name,param.requires_grad)\n",
    "    #param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn(1, 3, 124, 124)\n",
    "output = layer(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrderedDict+Sequential, 讓我們替每一個module命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "ReLU()\n",
      "Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "ReLU()\n"
     ]
    }
   ],
   "source": [
    "for module in layer.modules():\n",
    "    print(module)\n",
    "    #param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "conv1.bias True\n",
      "conv2.weight True\n",
      "conv2.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in layer.named_parameters():\n",
    "    print(name,param.requires_grad)\n",
    "    #param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn(1, 1, 124, 124)\n",
    "output = layer(input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append 新的 module到 sequential上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "modules = []\n",
    "modules.append(nn.Conv2d(1,20,5))\n",
    "modules.append(nn.ReLU())\n",
    "modules.append(nn.Conv2d(20,64,5))\n",
    "modules.append(nn.ReLU())\n",
    "\n",
    "layer = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn(1, 1, 124, 124)\n",
    "output = layer(input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 另一種方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Sequential()\n",
    "layer.add_module(\"conv1\", nn.Conv2d(1,20,5))\n",
    "layer.add_module(\"relu1\", nn.ReLU())\n",
    "layer.add_module(\"conv2\", nn.Conv2d(20,64,5))\n",
    "layer.add_module(\"relu2\", nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn(1, 1, 124, 124)\n",
    "output = layer(input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleList\n",
    "* 操作就像是python list, 但其內的module, parameters是可以被追蹤的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.ModuleList()\n",
    "layer.append(nn.Conv2d(1,20,5))\n",
    "layer.append(nn.ReLU())\n",
    "layer.append(nn.Conv2d(20,64,5))\n",
    "layer.append(nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn(1, 1, 124, 124)\n",
    "for _, module in enumerate(layer):\n",
    "    if _ == 0:\n",
    "        output = module(input_)\n",
    "    else:\n",
    "        output = module(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以追蹤是什麼意思？ nn.Module有辦法去獲取ModuleList裡面的資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.layer.append(nn.Conv2d(1,20,5))\n",
    "        self.layer.append(nn.ReLU())\n",
    "        self.layer.append(nn.Conv2d(20,64,5))\n",
    "        self.layer.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.layer:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer): ModuleList(\n",
       "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 116, 116])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randn(1, 1, 124, 124)\n",
    "output = model(input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果是一般的 python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer = []\n",
    "        self.layer.append(nn.Conv2d(1,20,5))\n",
    "        self.layer.append(nn.ReLU())\n",
    "        self.layer.append(nn.Conv2d(20,64,5))\n",
    "        self.layer.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.layer:\n",
    "            x = module(x)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model()"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
